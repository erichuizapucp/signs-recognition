
\chapter{Introduction and planning} \label{ch-1}

\section{Introduction}

The World Health Organization (WHO) stated that 466 million people worldwide have disabling hearing loss posing a global cost of 750 million dollars annually for unaddressed cases, estimating that by 2050 over 900 million people will have this condition. Current estimates suggest 83\% gap in hearing aid needed and use i.e., only 17\% of those who could benefit from using some kind of hearing aid\cite{deafness_and_hearing_loss_2020}, configuring a real need for investing on inclusive computing system that help on decreasing the gap and enable people with the tools to successfully interact with computer systems.

The Peruvian Institute of Informatics and Statistics (INEI) conducted a national disabilities survey with the objective of developing a better understanding about disabilities that affect the Peruvian population as well as their prevalence and posterior segmentation\cite{disabilities_survey_2012}. Results showed that 1.8\% of the Peruvian population suffer at least partially when not permanent deafness or hearing limitations indicating that approximately half a million people in the country is subject to have impediments or difficulties while performing day to day tasks that require the assistance of computer based systems. Peruvians with deafness or hearing limitations use the Peruvian Signs Language (PSL) as their main communication medium. PSL is mandatory at universities and certain public institutions as well as some official acts but is not common or nonexistent in the majority of venues and acts, henceforth the importance of designing systems that are capable to synthesize PSL inputs and simulate or generate PSL compatible outputs in a natural way. Furthermore, it is worth to indicate that in the same way as spoken languages, signs languages present local variations i.e. people living in Lima metropolitan area are not expected to use the same set of signs as people in other parts of the territory thereby it is correct to assume the existence of local PSL variations that enrich the language but at the same time add complexity towards to standardization, description and processing. This work will use a dataset containing PSL recordings with the variation used in the Lima metropolitan area due to the difficulty or inability to find datasets for other PSL variations.

The Grammar and Signs research group of the Pontifical Catholic University of Peru (PUCP) built the first PSL dataset  which is publicly available at the university digital archives. The dataset consists of a set of recordings following a method that involves a signer, a translator and a coordinator. It is important to highlight that the dataset is neither labeled or annotated and cannot be used as it is for training or testing a machine learning model. This work elaborates a method for extracting features and learning PSL elements using a self supervised approach.

A grammatically description of the PSL elements including but not limited to phonology, dactylology, numbers, common expressions, gender, verb tenses, question forms, nouns and adjectives then continued with a definition of PSL manual and non manual features where manual features refers to hand gestures and non manual refers to other body parts sections like neck, shoulders and the head. It is important to establish the relation between grammatical rules and manual or non manual features for which we will use the HamNoSys / SiGML notation in the context of an isolated signs detection method which is then extended to a continuous signs detection method approached as a self supervised learning task focused on learning features from the PSL dataset using representational autoencoders and energy functions (LeCun et al. 2020) while reconstructing PSL elements.

A conversational user interface between deaf and a computing system enabled by a graphical gestural user interface powered by HAmin for a 3D human simulation achieved by synthesizing PSL to Spanish and then Spanish to a PSL bundled into a mobile application that can be installed as a standalone application or an extension to existing application or services.

This doctoral project pursues the main objective of building "A novel solution for deaf users who aim to improve their human-computer interaction enabled by state of art self supervised learning and computer vision methods." and the following specific objectives:

\begin{itemize}
\item Produce an efficient self supervised method for annotating small non annotated signs language datasets applied to the Peruvian signs language corpus produced by the Grammar and Signs research group of the Pontifical Catholic University of Peru generalizable to other sign languages datasets.
\item Produce a Nielsen heuristics aware graphical gestural user interface capable of enabling an effective bidirectional human computer interaction between deaf users and a device powered by an algorithm trained with the dataset produced in the objective above.
\item Produce a prototype installable and distributable application powered by the algorithm and user interface produced in objectives 1 and 2.
\end{itemize}


The rest of the document is organized as follows. In section \ref{literature-review} we review the related work and state-of-art on sign languages recognition, self supervised learning applied to images and videos and graphical gestural user interfaces. In sections \ref{research-schedule} and \ref{research-budget} we describe the schedule and budget plan for executing the set of tasks needed to complete the doctoral project and finally in section \ref{available-datasets} we present the available datasets for signs language recognition and their details.


\section{Literature Review}\label{literature-review}
\subsection{Self Supervised Learning}
Predicting the future or reconstructing the past is a way to learn and understanding the hidden patterns and distributions in data which is not necessarily annotated or pre-processed. Schmidhuber proposed an early self supervised method \cite{schmidhuber1990making} where two fully self supervised recurrent neural networks predicted future reinforcement based on external interactions and state sent to the next network cell is calculated on differences on successive predictions achieving a system capable to generate supervision signals. Unlike supervised tasks where predictions are influenced by target and observed variables, self supervised tasks are influenced in a greater extent by hidden or invisible variables also known as latent variables. By capturing those dependencies, a model can be used to answer questions about the values of unknown variables given the values of known variables. Energy-Based Models (EBMs) capture dependencies by associating a scalar energy (a measure of compatibility) to each configuration of the variables. Inference, i.e., making a prediction or decision, consists in setting the value of observed variables and finding values of the remaining variables that minimize the energy. Learning consists in finding an energy function that associates low energies to correct values of the remaining variables, and higher energies to incorrect values. A loss functional, minimized during learning, is used to measure the quality of the available energy functions. Within this common inference/learning framework, the wide choice of energy functions and loss functionals allows for the design of many types of statistical models, both probabilistic and non-probabilistic \cite{lecun2006tutorial}.

Unsupervised visual representation learning, or self-supervised learning, aims at obtaining features without using manual annotations and is rapidly closing the performance gap with supervised pre- training in computer vision. Many recent state-of-the-art methods build upon the instance discrimination task that considers each image of the dataset (or “instance”) and its transformations as a separate class. This task yields representations that are able to discriminate between different images, while achieving some invariance (latent variables) to image transformations. Recent self-supervised methods that use instance discrimination rely on a combination of two elements: a contrastive loss and a set of image transformations. The contrastive loss removes the notion of instance classes by directly comparing image features while the image transformations define the invariances encoded in the features. Both elements are essential to the quality of the resulting networks \cite{caron2020unsupervised}. Caron et al 2020 proposed a cluster based (non constrastive or architectural) approach for learning latent variables or invariance from non annotated datasets that is extendable to video datasets called SwAV (Swapping Assignments between views). SwAV uses an online clustering mechanism that is capable to scale well to large datasets including video by solving an optimal transport problem using the Sinkhorn distance. 

\subsection{Signs Language Recognition and annotation}
Signs Language Recognition (SLR) are roughly divided in two categories Isolated SLR and Continuous SLR\cite{adaloglou2020comprehensive} figure \ref{fig-slr} shows tasks involved on SLR.

\subsubsection{Isolated Signs Language Recognition}
Methods using this approach will assume individual and well defined glosses that are recognized in a series of video frames. Recognizing gestures is a difficult task, due to intrapersonal and interpersonal variations in performing them. In \cite{kennaway2015avatar} a new descriptive language called SiGML (Signing Gesture Markup Language) is presented, SiGML was developed from HamNoSys, the Hamburg Notation System. HamNoSys is a notation for recording sign language gestures, developed by researchers on sign language at the IDGS (Institut für Deutsche Ge-bädensprache) at the University of Hamburg, it is not specific to any one sign language, but is intended to cover all signing gestures in all sign languages. It records sign languages elements in terms of hand shape, hand location and hand movement and other special terms for non manual gestures. Signs are not holistic gestures but are rather analyzable, as a combination of linguistically significant features. Similarly to spoken languages, Signs Languages are composed of the following indivisible features\cite{adaloglou2020comprehensive}:
\begin{itemize}
\item Manual features, i.e. hand shape, position, movement, orientation of the palm or fingers
\item Non-manual features, namely eye gaze, head-nods/ shakes, shoulder orientations, various kinds of facial expression as mouthing and mouth gestures.
\end{itemize}
SiGML provides a formal notation that is easily understood by computer because it is XML based as shown in the figure \ref{fig-sigml}.

\subsubsection{Continuous Signs Language Recognition}
Continuous Signs Language Recognition (CSLR) uses contextual information that is not limited to spatial information that resides around the hand shape for fixed point in time, but also temporal information that consists of hand and body movements. Signs are famously multi-channel, information is carried in the hand shape, motions, body pose and even facial gestures \cite{slimane2021context}, \cite{camgoz2017subunets}. While models that recognize text based language use punctuation marks to separate sentences, CSLR models use pauses e.g. silent regions. There have been studies in the literature addressing automatic sign segmentation. However to the best of the authors’ knowledge, there is no study which utilizes sign segmentation for realizing continuous sign language recognition for complex scenarios. Following successful segmentation, the system needs to under-stand what information is being conveyed within a sign sentence. Current approaches tackle this by recognizing sign glosses and other linguistic components. Such methods can be grouped under the banner of CSLR. From a computer vision perspective, this is the most challenging task. Considering the input of the system is high dimensional spatio-temporal data, i.e. sign videos, models are required that understand what a signer looks like and how they interact and move within their 3D signing space. Moreover, the model needs to comprehend what these aspects mean in combination. This complex modeling problem is exacerbated by the asynchronous multi-articulatory nature of sign languages. Although there have been promising results towards CSLR, the state-of-the-art can only recognize sign glosses and operate within a limited domain of discourse, namely weather forecasts \cite{camgoz2020sign}

\subsubsection{Sign Languages Recognition Approaches}
\textbf{Specialized sub-networks (SubUNets)} allow decomposing a complex signs recognition problem into a series of specialized expert systems allowing to learn both spatial and temporal features where each subunit learns an intermediate representation that is transferred to the next subunit. (Camgoz et al.) \cite{camgoz2017subunets} proposed an architecture where each SubUNet consists of three tiers, firstly Convolutional Neural Network layer to learn spatial features, then a Bidirectional Long Short Term Memory layer to learn temporal features on top of the spatial features and finally a Connectionist Temporal Classification Loos layer to allow the networks to be trained with different length videos and label sequences as presented in figure \ref{temporal-modeling}. (Huang et al.) \cite{huang2018videobased} proposed a two streams \textbf{3DCNN} architecture for learning global (entire video frame) and local (hands area crop) video representations in combination with a Hierarchical Attention Network (HAN) for latent space based recognition eliminating the need for temporal layers as proposed in other studies \cite{slimane2021context}, \cite{camgoz2017subunets}, \cite{camgoz2020sign}. HAN is an extension of LSTM which incorporates the attention mechanism based on the structure of input. It uses a latent space which maps video representations learned on the 3DCNN streams with relevant video sentences using one hot encoding vectors. The encoder in the HAN reflects the hierarchical structures in its inputs and incorporates the attention mechanism.

\subsection{Graphical Gestural User Interfaces}
A subset of Natural Language Processing called natural language generation or simply generation produces sentences from semantic information. The lack of a written form for most sign languages means that these traditional processing stages work somewhat differently. Instead of a written string, many sign language systems will create some type of script (generally in a proprietary format for each system) that specifies the movements for an animated character. Instead of speech output, sign language systems produce an animation of a humanlike character performing the sentence (based on the information encoded in the script) \cite{huenerfauth2009sign}. HamNoSys defines a notation which is independent of the Signs Languages. A HamNoSys transcription is independent of the person or avatar performing it which solves the problem of different signers styles while signing. It records only those aspects of the gesture which are significant for the correct performance of a sign. HamNoSys defines a Signing space, see figure \ref{fig-signing-space} and a set of pre-defined locations see figure \ref{fig-signing-locations} on the surface of the body figures \cite{kennaway2015avatar}

To generate motion data for a particular avatar, numerical information about the avatar must be supplied, to be combined with the avatar- independent description of the movement, to produce avatar-specific motion data. The information required divides into two classes, information about the geometry of a specific avatar, and information about body language or signing style, which can vary independently of the avatar.
The geometrical information required about the avatar consists of:
\begin{itemize}
\item The dimensions of the avatar’s skeleton: the lengths of its bones, and the positions and orientations of the bones in some standard posture.
\item For each of the non-manuals defined in HamNoSys, the mixture of avatar-specific facial morphs or bone rotations which implements it.
\item The coordinates of every location nameable in HamNoSys, relative to the bone which moves the relevant body part. In principle, a HamNoSys location should be considered as a volume having a certain extent, the choice of which point within that volume to use being dependent on the context, but for implementation we have preferred to model each location by a single fixed point.
\end{itemize}

The first of these can in principle be read automatically from whatever avatar description file is exported by the modeling software used to create it. The second requires the avatar creator to express each of the HamNoSys facial movements as a combination of the avatar’s morphs. The third requires the avatar creator to specify the coordinates of all the HamNoSys locations on the surface of the body. Some of these locations can be discovered automatically. For example, it is fairly easy to determine surface points on the palmar, dorsal, ulnar, and radial sides of each finger, level with the midpoint of the middle phalanx. Other points are best placed manually: it is easier for the modeler to say where the ears are than to determine them algorithmically from analysis of the surface mesh. Specifying the locations of these surface points should be considered a necessary part of the construction of any avatar intended for synthetic animation. Note that no other knowledge is required of the surface mesh itself, which may be arbitrarily dense, subject to the requirement of rendering it at the desired frame rate.

The “body language” class of information consists of the mapping of HamNoSys categories to numbers. It requires:
\begin{itemize}
\item A numerical definition of the size of a “large”, “medium”, or “small” movement, “near” and “far” proximities, the shape of a “deep” or “shallow” curved arc, and similarly for all the other geometric categories in HamNoSys. Distances are best given as proportions of various measurements of the avatar: for example, “far” from the chest might be defined as a certain proportion of the length of either arm.
\item A numerical specification of the time required for each type of movement, and of how that time should change when HamNoSys specifies the manner of movement as “fast”, “slow”, “tense”, etc.
\item A numerical specification of how the hands should accelerate and decelerate during movements of different types, performed with different manners.
\item The temporal trajectories that the avatar-specific morphs and bone rotations that implement non-manual movements should follow. This takes the form of attack, sustain, and release times, plus a description of how the elements of the movement ramp up from zero to the full value during the attack, and down again to zero during the release.
\end{itemize}

Body language information is specific, not to a particular avatar body, but to the personality inhabiting it. Given all of this information, generating motion data requires solving the following problems \cite{kennaway2015avatar}.

\section{Research Schedule}\label{research-schedule}
\input{Tables/researchschedule}

\section{Research Budget}\label{research-budget}
The student counts with a USD 4000 education bonus per year provided by his employer. In the event the education bonus will not suffice the student will use his own monetary resources to cover additional expenses.
\input{Tables/researchbudget}

\section{Contributions}
This study will contribute with an annotated PSL dataset generalizable to other studies focused on accessibility for minorities and deaf population. Along with an annotated PSL dataset we will produce a set of state of art DNN architectures and models for learning features from a non annotated PSL dataset using self supervised techniques. The produced models will be compared using an ablative analysis focusing on indicators for Isolated Signs Language Recognition and Continuous Sign Language Recognition.  Additionally to Isolated SLR and Continuous SLR recognition models we will produce another set of state of art architectures and models for Sign Language Generation. These models will be trained to produce (regression) SiGML or HamNoSys notation from a Isolated SLR or a Continuous  SLR input. It is important to highlight that models designed for Sign Language Recognition and Sign Language Generation can be trained and used independently as building blocks for other studies in Signs Languages related tasks but at the same time can complement each other producing a pipeline that enables generating Signs Language elements from a non annotated dataset.

Finally we will animate the generated SiGML or HamNoSys Sign Language elements into a 3D animated avatar following state of art Human-Computer interaction techniques. Sign Language animation will allow closing the bridge between deaf people and computer based systems allowing a bi-directional communication and interaction.



\section{Target Indexed Journals}
\input{Tables/targetindexedjournals}